{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image processing with pytorhc notebook\n",
    "\n",
    "mean = [0.49159095, 0.48234567, 0.4467194 ]\n",
    "std = [0.23834856, 0.23494388, 0.25271425]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to datasets/cifar10/train/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a204efc52ad0400095eae2a64fc86aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='datasets/cifar10/train',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='datasets/cifar10/train',\n",
    "                                       train=False,\n",
    "                                       download=True,\n",
    "                                       transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=16,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = trainset.classes\n",
    "\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, labels = iter(trainloader).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 images in batch, each image is multi-channel (RGB) with 32px height and 32px width\n",
    "# 16 labels correspond to 16 images\n",
    "\n",
    "image_batch.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_label(index):\n",
    "    return trainset.classes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_label(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = 3 # 3 channels\n",
    "\n",
    "# 2 convolutional layers\n",
    "\n",
    "# first layer generates 16 feature maps\n",
    "hid1_size = 16 \n",
    "\n",
    "# second layer generates 32 feature maps\n",
    "hid2_size = 32\n",
    "\n",
    "\n",
    "# 2 linear layers\n",
    "\n",
    "# Layer 1: 32 x 32 => 28 x 28 => pooling => 14 x 14\n",
    "# Layer 2: 14 x 14 => 10 x 10 => pooling => 5 x 5\n",
    "# 32 x 5 x 5 = 800\n",
    "\n",
    "# first\n",
    "out1_size = 400\n",
    "\n",
    "# second layer correspond to 10 classes: trainset.classes\n",
    "out2_size = 10\n",
    "\n",
    "# 5x5 kernel\n",
    "k_conv_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, hid1_size, k_conv_size), # 28 x 28\n",
    "            nn.BatchNorm2d(hid1_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2)) # 14 x 14\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(hid1_size, hid2_size, k_conv_size), # 10 x 10\n",
    "            nn.BatchNorm2d(hid2_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2)) # 5 x 5\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hid2_size * k_conv_size * k_conv_size, out1_size),\n",
    "            nn.Sigmoid(),\n",
    "            # randomly turn of certain neurons,\n",
    "            # forcing other neurons to learn significant features\n",
    "            # mitigate neural network overfitting\n",
    "            nn.Dropout(0.5), # 800 x 0.5 = 400 = out1_size\n",
    "            nn.Linear(out1_size, out2_size))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x) \n",
    "        out = self.layer2(out) \n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        return F.log_softmax(out, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 \n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(trainloader)\n",
    "num_epochs = 20 \n",
    "loss_values = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # loop over every image in batch\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 2000 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss {:.4f}'\n",
    "                .format(epoch + 1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "            loss_values.append(loss.item())\n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (range(1, num_epochs+1))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.plot(x, loss_values)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in testloader:\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    print('Accuracy of the model on 10000 test images: {}%'\n",
    "             .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images, sample_labels = testset[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = np.transpose(sample_images, (1, 2, 0))\n",
    "sample_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, M = sample_image.min(), sample_image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = (1/(abs(m) * M)) * sample_image + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img, test_label = testset[23]\n",
    "\n",
    "test_img = test_img.reshape(-1, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_predicted = model(test_img)\n",
    "_, predicted = torch.max(out_predicted.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual label: ', get_text_label(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicted label: ', get_text_label(predicted.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
